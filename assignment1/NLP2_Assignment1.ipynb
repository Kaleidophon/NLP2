{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(\"training/hansards.36.2.e\", \"r\", encoding=\"utf8\") \n",
    "english_sentences = [ ['-NULL-'] + line.strip().lower().split() for line in f.readlines()]\n",
    "f.close()\n",
    "f = open(\"training/hansards.36.2.f\", \"r\", encoding=\"utf8\")\n",
    "french_sentences = [line.strip().lower().split() for line in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231164\n"
     ]
    }
   ],
   "source": [
    "translations = list(zip(english_sentences, french_sentences))\n",
    "N = len(translations)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 %\n"
     ]
    }
   ],
   "source": [
    "## this cell computes the RFEs (takes ~15 mins)\n",
    "\n",
    "word_pair_counts = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "# count the occurences of each french word given the english word\n",
    "for t, (english, french) in enumerate(translations):\n",
    "        \n",
    "    # print percentage    \n",
    "    clear_output(wait=True)\n",
    "    print('{:0.0f}'.format(t/N*100), '%')\n",
    "    \n",
    "    # create list of possible alignments\n",
    "    e_length = len(english)\n",
    "    f_length = len(french)\n",
    "    alignments = list(product(range(e_length), range(f_length)))\n",
    "    \n",
    "    # count word pair occurences\n",
    "    for e_i, f_i in alignments:\n",
    "        english_word = english[e_i]\n",
    "        french_word = french[f_i]\n",
    "        \n",
    "        word_pair_counts[english_word][french_word] += 1\n",
    "\n",
    "rfe = dict(word_pair_counts)\n",
    "\n",
    "# compute the RFEs (divide the (f|e) word count by the total count of the (e) word)\n",
    "for english_word, french_word_counts in rfe.items():\n",
    "    total_occurences = sum(french_word_counts.values())\n",
    "    for french_word in rfe[english_word]:\n",
    "        word_count = rfe[english_word][french_word]\n",
    "        rfe[english_word][french_word] = word_count / total_occurences\n",
    "        \n",
    "# save to pickle\n",
    "pickle.dump(rfe, open('pickles/rfe.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# quick check if everything sums (approx) to 1\n",
    "rfe = pickle.load(open('pickles/rfe.pkl', 'rb'))\n",
    "\n",
    "not_one = []\n",
    "for _, probs in rfe.items():\n",
    "    prob_sum = sum(probs.values())\n",
    "    if prob_sum != 1.0:\n",
    "        not_one.append(prob_sum)\n",
    "not_one = np.array(not_one)\n",
    "print(len(not_one[not_one < 0.99999999999]) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM1 EM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alignment_indices(english_sentence, french_sentence):\n",
    "    e_length = len(english_sentence)\n",
    "    f_length = len(french_sentence)\n",
    "    alignments = list(product(range(e_length), range(f_length)))\n",
    "    return alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_perplexity(theta_dict, aligned_sentences):\n",
    "    logprobs = []\n",
    "    total_sum = 0\n",
    "    for english_sentence, french_sentence in aligned_sentences:\n",
    "        french_sum = 0\n",
    "        for j in range(len(french_sentence)): \n",
    "            f_j = french_sentence[j]\n",
    "            log_sum = []\n",
    "            for i in range(len(english_sentence)): \n",
    "                e_i = english_sentence[i]\n",
    "                log_sum.append(theta_dict[e_i][f_j])\n",
    "            french_sum += np.log(np.sum(log_sum))\n",
    "        total_sum += french_sum\n",
    "    perplexity = total_sum\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "sub = 500\n",
    "\n",
    "french_vocab = list(set([word for sublist in french_sentences[:sub] for word in sublist]))\n",
    "english_vocab = list(set([word for sublist in english_sentences[:sub] for word in sublist]))\n",
    "\n",
    "# initialize theta_0 uniformly\n",
    "theta_0 = 1 / len(english_vocab)\n",
    "\n",
    "theta_dict = defaultdict(lambda: defaultdict(lambda:theta_0))\n",
    "\n",
    "perplexities = []\n",
    "\n",
    "for t in range(iterations):\n",
    "    print('iteration', t)\n",
    "    # initialize all counts to 0\n",
    "    counts_dict = defaultdict(lambda: defaultdict(lambda: 0.))\n",
    "    total_f_dict = defaultdict(lambda: 0.)\n",
    "    total_e_dict = defaultdict(lambda: 0.)\n",
    "    \n",
    "    # E-step\n",
    "    for english, french in translations[:sub]:\n",
    "        for french_word in french:\n",
    "            total_f_dict[french_word] = 0.\n",
    "            for english_word in english:\n",
    "                total_f_dict[french_word] += theta_dict[english_word][french_word]\n",
    "#         for french_word in french:\n",
    "            for english_word in english:\n",
    "                value = theta_dict[english_word][french_word] / total_f_dict[french_word]\n",
    "                counts_dict[english_word][french_word] += value\n",
    "                total_e_dict[english_word] += value\n",
    "    \n",
    "    # M-step\n",
    "    for english_word in english_vocab:\n",
    "        for french_word in french_vocab:\n",
    "            theta_dict[english_word][french_word] = \\\n",
    "            counts_dict[english_word][french_word] / total_e_dict[english_word]\n",
    "            \n",
    "    # compute perplexity\n",
    "    perplexity = compute_perplexity(theta_dict, translations[:sub])\n",
    "    perplexities.append(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "theta_dict['the']\n",
    "prediction = max(theta_dict['the'].items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-45104.036833253725, -44352.782454743981, -44081.983069500246, -43909.097576131768, -43787.747249336047, -43699.674720551207, -43633.304812172326, -43581.647947084442, -43540.48899846898, -43507.132277936063]\n"
     ]
    }
   ],
   "source": [
    "print(perplexities)\n",
    "plt.plot(perplexities)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM1 Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_l = max([len(s) for s in english_sentences])\n",
    "max_m = max([len(s) for s in french_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-29f845a77f74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                     \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts_alignments_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts_alignments_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtheta_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "sub = 10000\n",
    "\n",
    "theta_dict = defaultdict(lambda: defaultdict(lambda: 0.1))\n",
    "q = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.1))))\n",
    "perps = []\n",
    "\n",
    "for s in range(iterations):\n",
    "    \n",
    "    print('iteration', s)\n",
    "    # initialize all counts to 0\n",
    "    \n",
    "    counts_words_dict = defaultdict(lambda: defaultdict(lambda: 0.))\n",
    "    counts_alignments_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.))))\n",
    "    count_f_dict = defaultdict(lambda: 0.)\n",
    "    count_e_dict = defaultdict(lambda: 0.)\n",
    "    \n",
    "    for english_sentence, french_sentence in translations[:sub]:\n",
    "        l = len(english_sentence)\n",
    "        m = len(french_sentence)\n",
    "        for i in range(m):\n",
    "            f_i = french_sentence[i]\n",
    "            delta_denominator = sum([q[j_k][i][l][m]*theta_dict[f_i][english_sentence[j_k]] for j_k in range(l)])\n",
    "            \n",
    "            for j in range(l):\n",
    "                e_j = english_sentence[j]\n",
    "                \n",
    "                delta = (q[j][i][l][m]*theta_dict[f_i][e_j]) / delta_denominator\n",
    "                \n",
    "                counts_words_dict[e_j][f_i] += delta\n",
    "                count_e_dict[e_j] += delta\n",
    "                counts_alignments_dict[l][m][i][j] += delta\n",
    "                \n",
    "#     for english_sentence, french_sentence in translations[:sub]:\n",
    "#         l = len(english_sentence)\n",
    "#         m = len(french_sentence)\n",
    "#         for i in range(m):\n",
    "#             for j in range(l):\n",
    "#                 f_i = french_sentence[i]\n",
    "#                 e_j = english_sentence[j]\n",
    "                \n",
    "#                 q[j][i][l][m] = counts_alignments_dict[l][m][i][j] / sum(counts_alignments_dict[l][m][i].values())\n",
    "#                 theta_dict[f_i][e_j] = counts_words_dict[e_j][f_i] / count_e_dict[e_j]\n",
    "                \n",
    "    for j in q.keys():\n",
    "        for i in q[j].keys():\n",
    "            for l in q[j][i].keys():\n",
    "                for m in q[j][i][l].keys():\n",
    "                    q[j][i][l][m] = counts_alignments_dict[l][m][i][j] / sum(counts_alignments_dict[l][m][i].values())\n",
    "    \n",
    "    for f_i in theta_dict.keys():\n",
    "        for e_j in theta_dict[f_i].keys():\n",
    "            theta_dict[f_i][e_j] = counts_words_dict[e_j][f_i] / count_e_dict[e_j]\n",
    "                \n",
    "    perp = compute_perplexity(theta_dict, translations[:sub])\n",
    "    perps.append(perp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perplexity plot IBM2\n",
    "print(perps)\n",
    "plt.figure()\n",
    "plt.plot(perps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of probs for 'the': 1.00000000000001\n",
      "predicted translation of 'the': ('le', 0.4485129388732313)\n",
      "\n",
      "sum of probs for 'le': 20.68241674978538\n",
      "predicted translation of 'le': ('right', 0.7126416713783849)\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "probs_for_the = sum([theta_dict[x]['the'] for x in theta_dict.keys() if 'the' in theta_dict[x].keys()])\n",
    "print('sum of probs for \\'the\\':', probs_for_the)\n",
    "\n",
    "translation_the = max([(x, theta_dict[x]['the']) for x in theta_dict.keys() if 'the' in theta_dict[x].keys()],\n",
    "                     key=lambda x: x[1])[0]\n",
    "print('predicted translation of \\'the\\':', (translation_the, theta_dict[translation_the]['the']))\n",
    "\n",
    "print('')\n",
    "\n",
    "probs_for_le = sum(theta_dict['le'].values())\n",
    "print('sum of probs for \\'le\\':', probs_for_le)\n",
    "\n",
    "translation_le = max(theta_dict['le'].items(), key=lambda x: x[1])[0]\n",
    "print('predicted translation of \\'le\\':', (translation_le, theta_dict['le'][translation_le]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
