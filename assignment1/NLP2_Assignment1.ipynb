{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pickle\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(\"training/hansards.36.2.e\", \"r\", encoding=\"utf8\") \n",
    "english_sentences = [ ['-NULL-'] + line.strip().lower().split() for line in f.readlines()]\n",
    "f.close()\n",
    "f = open(\"training/hansards.36.2.f\", \"r\", encoding=\"utf8\")\n",
    "french_sentences = [line.strip().lower().split() for line in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231164\n"
     ]
    }
   ],
   "source": [
    "translations = list(zip(english, french))\n",
    "N = len(translations)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 %\n"
     ]
    }
   ],
   "source": [
    "## this cell computes the RFEs (takes ~15 mins)\n",
    "\n",
    "word_pair_counts = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "# count the occurences of each french word given the english word\n",
    "for t, (english, french) in enumerate(translations):\n",
    "        \n",
    "    # print percentage    \n",
    "    clear_output(wait=True)\n",
    "    print('{:0.0f}'.format(t/N*100), '%')\n",
    "    \n",
    "    # create list of possible alignments\n",
    "    e_length = len(english)\n",
    "    f_length = len(french)\n",
    "    alignments = list(product(range(e_length), range(f_length)))\n",
    "    \n",
    "    # count word pair occurences\n",
    "    for e_i, f_i in alignments:\n",
    "        english_word = english[e_i]\n",
    "        french_word = french[f_i]\n",
    "        \n",
    "        word_pair_counts[english_word][french_word] += 1\n",
    "\n",
    "rfe = dict(word_pair_counts)\n",
    "\n",
    "# compute the RFEs (divide the (f|e) word count by the total count of the (e) word)\n",
    "for english_word, french_word_counts in rfe.items():\n",
    "    total_occurences = sum(french_word_counts.values())\n",
    "    for french_word in rfe[english_word]:\n",
    "        word_count = rfe[english_word][french_word]\n",
    "        rfe[english_word][french_word] = word_count / total_occurences\n",
    "        \n",
    "# save to pickle\n",
    "pickle.dump(rfe, open('pickles/rfe.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# quick check if everything sums (approx) to 1\n",
    "rfe = pickle.load(open('pickles/rfe.pkl', 'rb'))\n",
    "\n",
    "not_one = []\n",
    "for _, probs in rfe.items():\n",
    "    prob_sum = sum(probs.values())\n",
    "    if prob_sum != 1.0:\n",
    "        not_one.append(prob_sum)\n",
    "not_one = np.array(not_one)\n",
    "print(len(not_one[not_one < 0.99999999999]) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM1 EM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignment_indices(english_sentence, french_sentence):\n",
    "    e_length = len(english_sentence)\n",
    "    f_length = len(french_sentence)\n",
    "    alignments = list(product(range(e_length), range(f_length)))\n",
    "    return alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(theta_dict, aligned_sentences):\n",
    "    for english_sentence, french_sentence in aligned_sentences:\n",
    "        alignments = alignment_indices(english_sentence, french_sentence)\n",
    "        prob = 0.\n",
    "        for e_i, f_i in alignments:\n",
    "            # ???\n",
    "            prob *= theta_dict[french_sentence[f_i]][english_sentence[e_i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "iteration 40\n",
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n",
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "\n",
    "french_vocab = list(set([word for sublist in french_sentences[:100] for word in sublist]))\n",
    "english_vocab = list(set([word for sublist in english_sentences[:100] for word in sublist]))\n",
    "\n",
    "# initialize theta_0 uniformly\n",
    "theta_0 = 1 / len(french_vocab)\n",
    "\n",
    "theta_dict = defaultdict(lambda: defaultdict(lambda:theta_0))\n",
    "\n",
    "for k in range(iterations):\n",
    "    print('iteration', k)\n",
    "    # initialize all counts to 0\n",
    "    counts_dict = defaultdict(lambda: defaultdict(lambda: 0.))\n",
    "    total_f_dict = defaultdict(lambda: 0.)\n",
    "    total_e_dict = defaultdict(lambda: 0.)\n",
    "    \n",
    "    # E-step\n",
    "    for english, french in translations[:100]:\n",
    "        for french_word in french:\n",
    "            total_f_dict[french_word] = 0.\n",
    "            for english_word in english:\n",
    "                total_f_dict[french_word] += theta_dict[french_word][english_word]\n",
    "        for french_word in french:\n",
    "            for english_word in english:\n",
    "                value = theta_dict[french_word][english_word] / total_f_dict[french_word]\n",
    "                counts_dict[french_word][english_word] += value\n",
    "                total_e_dict[english_word] += value\n",
    "    \n",
    "    # M-step\n",
    "    for english_word in english_vocab:\n",
    "        for french_word in french_vocab:\n",
    "            theta_dict[french_word][english_word] = \\\n",
    "            counts_dict[french_word][english_word] / total_e_dict[english_word]\n",
    "            \n",
    "    # compute perplexity\n",
    "    #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4372456312459352 0.7160576192444245\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "theta_dict['le']\n",
    "prediction = max(theta_dict['le'].items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "print(theta_dict['le']['new'], theta_dict['le']['the'])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM1 Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
