{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>**IBM Model 1**</h1>\n",
    "\n",
    "1. a) Implement EM training (Brown et al., 1993) for IBM model 1; <br />\n",
    "    b) Implement variational inference for Bayesian IBM model 1; <br />\n",
    "    c) All of the tasks below should be performed for both models.<br />\n",
    "2. Plot the evolution of training log likelihood (or ELBO) as a function of the iteration.\n",
    "3. Plot the evolution of alignment error rate (AER) on validation data as a function of the iteration;\n",
    "4. Experiment with two criteria for model selection (i.e. deciding on number of training iterations): \n",
    "    1) convergence in terms of training log likelihood; \n",
    "    2) best AER on validation data;\n",
    "5. For the selected models, obtain Viterbi alignments for every sentence pair in a test\n",
    "corpus and compute AER using a gold-standard provided by the assistant;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import aer\n",
    "from collections import defaultdict, Counter\n",
    "from math import log2\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import progressbar\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import digamma, gammaln "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read training source corpus\n",
      "Read training target corpus\n",
      "Initialising parameters...\n",
      "Iteration #0 out of 9\n",
      "Doing E-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2000 of 2000) |####################| Elapsed Time: 0:00:01 Time:  0:00:01\n",
      "100% (2444 of 2444) |####################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing M-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% (203 of 2000) |##                   | Elapsed Time: 0:00:00 ETA:   0:00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: -48081.1072025\n",
      "Iteration #1 out of 9\n",
      "Doing E-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2000 of 2000) |####################| Elapsed Time: 0:00:01 Time:  0:00:01\n",
      "100% (2444 of 2444) |####################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing M-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16% (330 of 2000) |###                  | Elapsed Time: 0:00:00 ETA:   0:00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: -29819.011815\n",
      "Iteration #2 out of 9\n",
      "Doing E-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2000 of 2000) |####################| Elapsed Time: 0:00:01 Time:  0:00:01\n",
      "100% (2444 of 2444) |####################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing M-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5% (102 of 2000) |#                    | Elapsed Time: 0:00:00 ETA:   0:00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: -22276.9701882\n",
      "Iteration #3 out of 9\n",
      "Doing E-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2000 of 2000) |####################| Elapsed Time: 0:00:01 Time:  0:00:01\n",
      " 70% (1733 of 2444) |##############      | Elapsed Time: 0:00:00 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing M-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2444 of 2444) |####################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      " 11% (228 of 2000) |##                   | Elapsed Time: 0:00:00 ETA:   0:00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: -18854.6144618\n",
      "Iteration #4 out of 9\n",
      "Doing E-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2000 of 2000) |####################| Elapsed Time: 0:00:01 Time:  0:00:01\n",
      "100% (2444 of 2444) |####################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing M-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15% (304 of 2000) |###                  | Elapsed Time: 0:00:00 ETA:   0:00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: -17127.542811\n",
      "Iteration #5 out of 9\n",
      "Doing E-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2000 of 2000) |####################| Elapsed Time: 0:00:01 Time:  0:00:01\n",
      "100% (2444 of 2444) |####################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing M-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13% (279 of 2000) |##                   | Elapsed Time: 0:00:00 ETA:   0:00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: -16148.9557449\n",
      "Iteration #6 out of 9\n",
      "Doing E-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2000 of 2000) |####################| Elapsed Time: 0:00:01 Time:  0:00:01\n",
      "100% (2444 of 2444) |####################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing M-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12% (254 of 2000) |##                   | Elapsed Time: 0:00:00 ETA:   0:00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: -15542.8407332\n",
      "Iteration #7 out of 9\n",
      "Doing E-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2000 of 2000) |####################| Elapsed Time: 0:00:01 Time:  0:00:01\n",
      "100% (2444 of 2444) |####################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing M-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12% (254 of 2000) |##                   | Elapsed Time: 0:00:00 ETA:   0:00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: -15142.5732291\n",
      "Iteration #8 out of 9\n",
      "Doing E-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2000 of 2000) |####################| Elapsed Time: 0:00:01 Time:  0:00:01\n",
      "100% (2444 of 2444) |####################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing M-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11% (228 of 2000) |##                   | Elapsed Time: 0:00:00 ETA:   0:00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: -14865.6773482\n",
      "Iteration #9 out of 9\n",
      "Doing E-step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2000 of 2000) |####################| Elapsed Time: 0:00:01 Time:  0:00:01\n",
      "100% (2444 of 2444) |####################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing M-step...\n",
      "perplexity: -14667.2393432\n",
      "0.45359749739311783\n"
     ]
    }
   ],
   "source": [
    "def read_corpus(file_name, source_language):\n",
    "    \"\"\"\n",
    "    Reads the corpus and saves each sentence in a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    with open(file_name, \"r\", encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            sentence = line.split()\n",
    "            \n",
    "            if source_language:\n",
    "                sentence.insert(0, \"null\")\n",
    "            corpus.append(sentence)\n",
    "    return corpus[:2000]\n",
    "\n",
    "\n",
    "def reduce_corpus(corpus):\n",
    "    flat_corpus = [word for sentence in corpus for word in sentence]\n",
    "    word_counts = Counter(flat_corpus)\n",
    "    small_corpus = []\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        small_sentence = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word_counts[word] != 1:\n",
    "                small_sentence.append(word)\n",
    "            else:\n",
    "                small_sentence.append(\"-LOW-\")\n",
    "        small_corpus.append(small_sentence)\n",
    "    return small_corpus\n",
    "    \n",
    "\n",
    "def initialise_parameters(source_corpus, target_corpus):\n",
    "    \"\"\"\n",
    "    Initialises the conditional probability of generating a source \n",
    "    word from a target word for all possible pairs of words in the source \n",
    "    and target sentences to 5 and then normalises the parameters such that \n",
    "    the initialisation is uniform.\n",
    "    \"\"\"\n",
    "    \n",
    "    flat_corpus = [word for sentence in source_corpus for word in sentence]\n",
    "    amount_source_words = len(set(flat_corpus))\n",
    "    theta0 = 1/amount_source_words\n",
    "    return defaultdict(lambda: defaultdict(lambda: theta0))\n",
    "\n",
    "\n",
    "def expectation_maximisation(source_corpus, target_corpus, parameters, \n",
    "                             num_iterations, min_perplexity_change):\n",
    "    \"\"\"\n",
    "    Do the EM algorithm until perplexity decreases very little or until \n",
    "    the number of iterations is reached.\n",
    "    \"\"\"\n",
    "    \n",
    "    old_perplexity = -100000\n",
    "    \n",
    "    for k in range(0, num_iterations):\n",
    "        print(\"Iteration #\" + str(k), \"out of\", num_iterations - 1)\n",
    "        counts_single = defaultdict(lambda: 1.0)\n",
    "        counts_pairs = defaultdict(lambda: defaultdict(float))\n",
    "        counts_alignments = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.))))\n",
    "        counts_single, counts_pairs = e_step(source_corpus, target_corpus,\n",
    "                                             parameters, counts_single, \n",
    "                                             counts_pairs)\n",
    "        parameters = m_step(parameters, counts_single, counts_pairs)\n",
    "        perplexity = compute_perplexity(parameters, source_corpus, target_corpus)\n",
    "        print(\"perplexity:\", perplexity)\n",
    "        \n",
    "        if abs(perplexity - old_perplexity) < min_perplexity_change:\n",
    "            return parameters\n",
    "        else:\n",
    "            old_perplexity = perplexity\n",
    "    return parameters\n",
    "    \n",
    "    \n",
    "def e_step(source_corpus, target_corpus, parameters, counts_single, \n",
    "           counts_pairs):\n",
    "    \"\"\"\n",
    "    Do the E-step by computing the expected counts.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Doing E-step...\")\n",
    "    \n",
    "    with progressbar.ProgressBar(max_value=len(target_corpus)) as bar:\n",
    "        for n in range(len(target_corpus)):\n",
    "            target_sentence = target_corpus[n]\n",
    "            source_sentence = source_corpus[n]\n",
    "\n",
    "            for i in range(len(target_sentence)):\n",
    "                normalisation_term = 0\n",
    "                target_word = target_sentence[i]\n",
    "\n",
    "                for j in range(len(source_sentence)):\n",
    "                    source_word = source_sentence[j]\n",
    "                    normalisation_term += parameters[source_word][target_word]\n",
    "                for j in range(len(source_sentence)):\n",
    "                    source_word = source_sentence[j]\n",
    "                    expected_count = parameters[source_word][target_word]/normalisation_term\n",
    "                    counts_pairs[source_word][target_word] += expected_count\n",
    "                    counts_single[source_word] += expected_count\n",
    "            bar.update(n)\n",
    "    return counts_single, counts_pairs\n",
    "\n",
    "\n",
    "def m_step(parameters, counts_single, counts_pairs):\n",
    "    \"\"\"\n",
    "    Do the M-step by normalising the parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Doing M-step...\")\n",
    "    \n",
    "    with progressbar.ProgressBar(max_value=len(counts_pairs)) as bar:\n",
    "        i = 0\n",
    "        for source_word, target_words in counts_pairs.items():\n",
    "            for target_word, expected_count in target_words.items():\n",
    "                parameters[source_word][target_word] = expected_count/counts_single[source_word]\n",
    "            i += 1\n",
    "            bar.update(i)\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def compute_perplexity(theta_dict, source_corpus, target_corpus):\n",
    "    logprobs = []\n",
    "    total_sum = 0\n",
    "    \n",
    "    for n in range(len(source_corpus)):\n",
    "        english_sentence = source_corpus[n]\n",
    "        french_sentence = target_corpus[n]\n",
    "        french_sum = 0\n",
    "        for j in range(len(french_sentence)): \n",
    "            f_j = french_sentence[j]\n",
    "            log_sum = []\n",
    "            for i in range(len(english_sentence)): \n",
    "                e_i = english_sentence[i]\n",
    "                log_sum.append(theta_dict[e_i][f_j])\n",
    "            french_sum += np.log(np.sum(log_sum))\n",
    "        total_sum += french_sum\n",
    "    perplexity = total_sum\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def calculate_likelihood(source_corpus, target_corpus, parameters):\n",
    "    \"\"\"\n",
    "    Calculates the likelihood over te corpus:\n",
    "    sum_<f,e> sum^m_i=1 log sum^l_j=0 pi_t(target|source) \n",
    "    \"\"\"\n",
    "    \n",
    "    corpus_likelihood = 0\n",
    "    \n",
    "    for n in range(len(source_corpus)):\n",
    "        source_sentence = source_corpus[n]\n",
    "        target_sentence = target_corpus[n]\n",
    "        target_likelihood = 0\n",
    "        \n",
    "        for i in range(len(target_sentence)):\n",
    "            target_word = target_sentence[i]\n",
    "            source_likelihood = 0\n",
    "            \n",
    "            for j in range(len(source_sentence)):\n",
    "                source_word = source_sentence[j]\n",
    "                source_likelihood += parameters[source_word][target_word]\n",
    "            target_likelihood += log2(source_likelihood)\n",
    "        corpus_likelihood += target_likelihood\n",
    "    return corpus_likelihood\n",
    "   \n",
    "    \n",
    "def get_best_alignment(source_corpus, target_corpus, parameters):\n",
    "    \"\"\"\n",
    "    Gets the best alignment for each sentence and saves the alignment\n",
    "    in a list of lists that holds tuples for each position in the sentence\n",
    "    and looks as follows:\n",
    "    (sentence_index, target_word_index, source_word_index).\n",
    "    \"\"\"\n",
    "    \n",
    "    alignments = []\n",
    "    \n",
    "    for n in range(len(source_corpus)):\n",
    "        source_sentence = source_corpus[n]\n",
    "        target_sentence = target_corpus[n]\n",
    "        alignment = []\n",
    "        \n",
    "        for i in range(len(target_sentence)):\n",
    "            target_word = target_sentence[i]\n",
    "            best_prob = 0\n",
    "            best_j = 0\n",
    "            \n",
    "            for j in range(len(source_sentence)):\n",
    "                source_word = source_sentence[j]\n",
    "                prob = parameters[source_word][target_word]\n",
    "                \n",
    "                if prob > best_prob:\n",
    "                    best_prob = prob\n",
    "                    best_j = j\n",
    "                    \n",
    "            if best_j != 0:    \n",
    "                alignment.append((n, best_j, i+1))\n",
    "        alignments.append(alignment)\n",
    "    return alignments\n",
    "\n",
    "\n",
    "def compute_aer(predictions):\n",
    "    gold_sets = aer.read_naacl_alignments(\"validation/dev.wa.nonullalign\")\n",
    "    metric = aer.AERSufficientStatistics()\n",
    "    \n",
    "    for gold, prediction in zip(gold_sets, predictions):\n",
    "        prediction = set([(alignment[1], alignment[2]) for alignment in prediction])\n",
    "        metric.update(sure=gold[0], probable=gold[1], predicted=prediction)\n",
    "    print(metric.aer())\n",
    "    \n",
    "    \n",
    "train_source_corpus = read_corpus(\"training/hansards.36.2.e\", True)\n",
    "train_source_corpus = reduce_corpus(train_source_corpus)\n",
    "print(\"Read training source corpus\")\n",
    "train_target_corpus = read_corpus(\"training/hansards.36.2.f\", False)\n",
    "train_target_corpus = reduce_corpus(train_target_corpus)\n",
    "print(\"Read training target corpus\")\n",
    "val_target_corpus = read_corpus(\"validation/dev.f\", False)\n",
    "val_source_corpus = read_corpus(\"validation/dev.e\", True)\n",
    "\n",
    "print(\"Initialising parameters...\")\n",
    "initial_params = initialise_parameters(train_source_corpus, train_target_corpus)\n",
    "parameters = expectation_maximisation(train_source_corpus, train_target_corpus, \n",
    "                                      initial_params, 10, 5)\n",
    "alignments = get_best_alignment(val_source_corpus, val_target_corpus, parameters)\n",
    "compute_aer(alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>IBM Model 2</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #0 out of 9\n",
      "-43825.6879894\n",
      "Iteration #1 out of 9\n",
      "-27689.9665298\n",
      "Iteration #2 out of 9\n",
      "-17920.2666\n",
      "Iteration #3 out of 9\n",
      "-14109.4795137\n",
      "Iteration #4 out of 9\n",
      "-13396.0014646\n",
      "Iteration #5 out of 9\n",
      "-13721.8589776\n",
      "Iteration #6 out of 9\n",
      "-14185.103931\n",
      "Iteration #7 out of 9\n",
      "-14544.6924467\n",
      "Iteration #8 out of 9\n",
      "-14789.8907897\n",
      "Iteration #9 out of 9\n",
      "-14957.5577081\n",
      "0.4627329192546584\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "sub = 1\n",
    "\n",
    "\n",
    "def expectation_maximisation2(source_corpus, target_corpus, parameters, num_iterations, min_perplexity_change):\n",
    "    q = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.1))))\n",
    "    old_perplexity = -100000\n",
    "    \n",
    "    for k in range(0, num_iterations):\n",
    "        print(\"Iteration #\" + str(k), \"out of\", num_iterations - 1)\n",
    "        \n",
    "        counts_pairs = defaultdict(lambda: defaultdict(lambda: 0.))\n",
    "        counts_alignments = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.))))\n",
    "        counts_single = defaultdict(lambda: 0.)\n",
    "        counts_pairs, counts_single, counts_alignments = e_step2(source_corpus, target_corpus, counts_pairs, counts_single, counts_alignments, q)\n",
    "        parameters, q = m_step2(parameters, q, counts_alignments, counts_pairs, counts_single)\n",
    "        perplexity = compute_perplexity(parameters, source_corpus, target_corpus)\n",
    "        print(perplexity)\n",
    "        \n",
    "        if abs(perplexity - old_perplexity) < min_perplexity_change:\n",
    "            return parameters\n",
    "        else:\n",
    "            old_perplexity = perplexity\n",
    "    return parameters        \n",
    "\n",
    "\n",
    "def e_step2(source_corpus, target_corpus, counts_pairs, counts_single, counts_alignments, q):\n",
    "    for n in range(len(source_corpus)):\n",
    "        source_sentence = source_corpus[n]\n",
    "        target_sentence = target_corpus[n]\n",
    "        l = len(source_sentence)\n",
    "        m = len(target_sentence)\n",
    "\n",
    "        for i, target_word in enumerate(target_sentence):\n",
    "            delta_denominator = sum([q[j_k][i][l][m]*parameters[source_sentence[j_k]][target_word] for j_k in range(l)])\n",
    "\n",
    "            for j, source_word in enumerate(source_sentence):\n",
    "                delta = (q[j][i][l][m]*parameters[source_word][target_word]) / delta_denominator\n",
    "\n",
    "                counts_pairs[source_word][target_word] += delta\n",
    "                counts_single[source_word] += delta\n",
    "                counts_alignments[l][m][i][j] += delta\n",
    "    return counts_pairs, counts_single, counts_alignments\n",
    "\n",
    "\n",
    "def m_step2(parameters, q, counts_alignments, counts_pairs, counts_single):\n",
    "    for j in q.keys():\n",
    "        for i in q[j].keys():\n",
    "            for l in q[j][i].keys():\n",
    "                for m in q[j][i][l].keys():\n",
    "                    q[j][i][l][m] = counts_alignments[l][m][i][j] / sum(counts_alignments[l][m][i].values())\n",
    "    \n",
    "    for source_word, target_words in parameters.items():\n",
    "        for target_word in target_words:\n",
    "            parameters[source_word][target_word] = counts_pairs[source_word][target_word]/counts_single[source_word]\n",
    "    return parameters, q\n",
    "\n",
    "\n",
    "parameters = initialise_parameters(train_source_corpus, train_target_corpus)\n",
    "parameters = expectation_maximisation2(train_source_corpus, train_target_corpus, parameters, \n",
    "                                       10, 5)\n",
    "alignments = get_best_alignment(val_source_corpus, val_target_corpus, parameters)\n",
    "compute_aer(alignments)\n",
    "\n",
    "# q = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.1))))\n",
    "# perps = []\n",
    "\n",
    "# for s in range(iterations):\n",
    "    \n",
    "#     print('iteration', s)\n",
    "#     # initialize all counts to 0\n",
    "    \n",
    "#     counts_pairs = defaultdict(lambda: defaultdict(lambda: 0.))\n",
    "#     counts_alignments = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.))))\n",
    "#     counts_single = defaultdict(lambda: 0.)\n",
    "    \n",
    "#     for n in range(len(train_source_corpus)):\n",
    "#         source_sentence = train_source_corpus[n]\n",
    "#         target_sentence = train_target_corpus[n]\n",
    "#         l = len(source_sentence)\n",
    "#         m = len(target_sentence)\n",
    "\n",
    "#         for i, target_word in enumerate(target_sentence):\n",
    "#             delta_denominator = sum([q[j_k][i][l][m]*parameters[source_sentence[j_k]][target_word] for j_k in range(l)])\n",
    "            \n",
    "#             for j, source_word in enumerate(source_sentence):\n",
    "#                 delta = (q[j][i][l][m]*parameters[source_word][target_word]) / delta_denominator\n",
    "                \n",
    "#                 counts_pairs[source_word][target_word] += delta\n",
    "#                 counts_single[source_word] += delta\n",
    "#                 counts_alignments[l][m][i][j] += delta\n",
    "                \n",
    "#     for j in q.keys():\n",
    "#         for i in q[j].keys():\n",
    "#             for l in q[j][i].keys():\n",
    "#                 for m in q[j][i][l].keys():\n",
    "#                     q[j][i][l][m] = counts_alignments[l][m][i][j] / sum(counts_alignments[l][m][i].values())\n",
    "    \n",
    "#     for source_word, target_words in parameters.items():\n",
    "#         for target_word in target_words:\n",
    "#             parameters[source_word][target_word] = counts_pairs[source_word][target_word]/counts_single[source_word]\n",
    "                \n",
    "#     perp = compute_perplexity(parameters, train_source_corpus, train_target_corpus)\n",
    "#     perps.append(perp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAD8CAYAAAC/1zkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nskahIiIiWaNQERGRrPn/wKVrk5G+BWoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f98d7404208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(perps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Bayes IBM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th iteration out of  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Urja\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:138: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "def elbo(theta_dict, lambda_dict, train_source_corpus, traing_target_corpus, alpha):\n",
    "\n",
    "    elbo_first_term = compute_perplexity(theta_dict, train_source_corpus, train_target_corpus)\n",
    "    elbo_second_term = 0\n",
    "\n",
    "    for english_word in english_vocab: \n",
    "        normalization = sum([lambda_dict[english_word][french_word] for french_word in french_vocab])\n",
    "        digamma_normalization = digamma(normalization)\n",
    "        gammaln_normalization = gammaln(normalization)\n",
    "        for french_word in french_vocab: \n",
    "            first_term = digamma(lambda_dict[english_word][french_word]) - digamma_normalization \n",
    "            first_term = first_term * (alpha - lambda_dict[english_word][french_word])\n",
    "            second_term = gammaln(lambda_dict[english_word][french_word]) \n",
    "            third_term = gammaln(alpha)\n",
    "            fourth_term = gammaln(len(french_vocab) * alpha)\n",
    "            elbo_second_term += first_term + second_term - third_term + fourth_term - gammaln_normalization\n",
    "\n",
    "    return elbo_first_term + elbo_second_term\n",
    "\n",
    "\n",
    "alpha = 1\n",
    "iterations = 10 \n",
    "\n",
    "theta_dict = initialise_parameters(train_source_corpus, train_target_corpus)\n",
    "lambda_dict = initialise_parameters(train_source_corpus, train_target_corpus)\n",
    "\n",
    "english_vocab = set([word for sentence in train_source_corpus for word in sentence])\n",
    "french_vocab = set([word for sentence in train_target_corpus for word in sentence])\n",
    "\n",
    "for i in range(iterations): \n",
    "    print(i, 'th iteration out of ', iterations-1)\n",
    "    counts_single = defaultdict(lambda: 1.0)\n",
    "    counts_pairs = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    # e step\n",
    "    for n in range(len(train_source_corpus)):\n",
    "        source_sentence = train_source_corpus[n]\n",
    "        target_sentence = train_target_corpus[n]\n",
    "        \n",
    "        for source_word in source_sentence: \n",
    "            normalization = sum([lambda_dict[source_word][target_word] for target_word in target_sentence])\n",
    "            for target_word in target_sentence: \n",
    "                theta_dict[source_word][target_word] = np.exp(digamma(lambda_dict[source_word][target_word]) - \\\n",
    "                                                              digamma(normalization))\n",
    "    # m step    \n",
    "    for source_word in english_vocab: \n",
    "        target_words = lambda_dict[source_word].keys()\n",
    "        for target_word in target_words:\n",
    "            lambda_dict[source_word][target_word] = alpha + theta_dict[source_word][target_word]\n",
    "    \n",
    "    elbo_iter = elbo(theta_dict, lambda_dict, train_source_corpus, train_target_corpus, alpha)\n",
    "    print(elbo_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sénateurs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46825060387463785"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator \n",
    "\n",
    "print(max(lambda_dict['the'].items(), key=operator.itemgetter(1))[0])\n",
    "lambda_dict['the']['sénateurs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-19030.957387138806"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_perplexity(theta_dict, train_source_corpus, train_target_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
