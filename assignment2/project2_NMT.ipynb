{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import os\n",
    "import progressbar\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "\n",
    "class EncoderRNN(nn.Module): \n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        word_embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(word_embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "    \n",
    "\n",
    "class EncoderPOS(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderPOS, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.positional = positional\n",
    "        self.word_embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.pos_embedding = nn.Embedding(MAX_LENGTH, hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.linear = nn.Linear(hidden_size*2, hidden_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        index = input[0]\n",
    "        word = input[1]\n",
    "        word_embedded = self.word_embedding(word).view(1, 1, -1)\n",
    "        pos_embedded = self.pos_embedding(torch.tensor([index], dtype=torch.long)).view(1, 1, -1)\n",
    "        \n",
    "        combined_embedding = torch.cat((word_embedded.squeeze(0), pos_embedded.squeeze(0)), 1)\n",
    "        output = self.dropout(combined_embedding)\n",
    "        output = self.linear(output)\n",
    "        return output, hidden            \n",
    "\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "\n",
    "    \n",
    "    \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "    \n",
    "    \n",
    "class NMT(nn.Module):\n",
    "    def __init__(self, i2w_english, i2w_french, hidden_size, positional, criterion):\n",
    "        super(NMT, self).__init__()\n",
    "        if positional:\n",
    "            self.encoder = EncoderPOS(len(i2w_french), hidden_size)\n",
    "        else:\n",
    "            self.encoder = EncoderRNN(len(i2w_french), hidden_size)\n",
    "        self.decoder = AttnDecoderRNN(hidden_size, len(i2w_english))\n",
    "        self.positional = positional\n",
    "        self.criterion = criterion\n",
    "    \n",
    "    \n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        loss = 0\n",
    "        \n",
    "        input_length = input_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "        \n",
    "        encoder_hidden = self.encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, self.encoder.hidden_size)\n",
    "        \n",
    "        if input_length > MAX_LENGTH: input_length = MAX_LENGTH\n",
    "            \n",
    "        if self.positional:\n",
    "            average_hidden = torch.zeros(1, self.encoder.hidden_size)\n",
    "            \n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden = self.encoder((ei, input_tensor[ei]), encoder_hidden)\n",
    "                average_hidden += encoder_output\n",
    "                encoder_outputs[ei] = encoder_output[0, 0]\n",
    "            encoder_hidden = (average_hidden/input_length).unsqueeze(0)\n",
    "        else:\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden = self.encoder(input_tensor[ei], encoder_hidden)\n",
    "                encoder_outputs[ei] = encoder_output[0, 0]            \n",
    "            \n",
    "        decoder_input = torch.tensor([[SOS_token_en]])\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        # Use teacher forcing\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        return loss.item()/target_length\n",
    "    \n",
    "    \n",
    "    def evaluate(self, w2i_french, i2w_english, input_sentence, target_sentence):\n",
    "        with torch.no_grad():         \n",
    "            input_tensor = sentence_to_tensor(w2i_french, input_sentence)\n",
    "            target_tensor = sentence_to_tensor(w2i_english, target_sentence)\n",
    "            \n",
    "            input_length = input_tensor.size()[0]\n",
    "            encoder_hidden = self.encoder.initHidden()\n",
    "\n",
    "            encoder_outputs = torch.zeros(MAX_LENGTH, self.encoder.hidden_size)\n",
    "\n",
    "            if input_length > MAX_LENGTH: input_length = MAX_LENGTH\n",
    "\n",
    "            if self.positional:\n",
    "                average_hidden = torch.zeros(1, self.encoder.hidden_size)\n",
    "\n",
    "                for ei in range(input_length):\n",
    "                    encoder_output, encoder_hidden = self.encoder((ei, input_tensor[ei]), encoder_hidden)\n",
    "                    average_hidden += encoder_output\n",
    "                    encoder_outputs[ei] += encoder_output[0, 0]\n",
    "                encoder_hidden = (average_hidden/input_length).unsqueeze(0)\n",
    "            else:\n",
    "                for ei in range(input_length):\n",
    "                    encoder_output, encoder_hidden = self.encoder(input_tensor[ei], encoder_hidden)\n",
    "                    encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "            decoder_input = torch.tensor([[SOS_token_en]])  # SOS\n",
    "\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            decoded_words = []\n",
    "            decoder_attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)\n",
    "\n",
    "            for di in range(MAX_LENGTH):\n",
    "                decoder_output, decoder_hidden, decoder_attention = self.decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                decoder_attentions[di] = decoder_attention.data\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                if topi.item() == EOS_token_en:\n",
    "                    decoded_words.append(\"<eos>\")\n",
    "                    break\n",
    "                else:\n",
    "                    decoded_words.append(i2w_english[topi.item()])\n",
    "\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    \"\"\"\n",
    "    Reads the data and returns it in a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    f = open(file_name, \"r\")\n",
    "    return [line.strip().split() for line in f.readlines()]\n",
    "\n",
    "\n",
    "def word_to_index(file_name):\n",
    "    \"\"\"\n",
    "    Obtains the vocabulary of a file and returns it \n",
    "    in a dictionary to be able to use w2i.\n",
    "    \"\"\"\n",
    "    \n",
    "    file = open(file_name) \n",
    "    w2i = json.load(file)\n",
    "    w2i[\"<sos>\"] = len(w2i)\n",
    "    return w2i\n",
    "\n",
    "\n",
    "def index_to_word(dictionary):\n",
    "    \"\"\"\n",
    "    Reverses the dictionary such that i2w can be used.\n",
    "    \"\"\"\n",
    "    \n",
    "    reversed_dict = {}\n",
    "    \n",
    "    for word, index in dictionary.items():\n",
    "        reversed_dict[index] = word\n",
    "    return reversed_dict\n",
    "\n",
    "\n",
    "def sentence_to_indices(w2i, sentence):\n",
    "    \"\"\"\n",
    "    Returns the indices of the words in a sentence in a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [w2i[word] for word in sentence]\n",
    "\n",
    "\n",
    "def sentence_to_tensor(w2i, sentence):\n",
    "    \"\"\"\n",
    "    Returns the tensor of a sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    indices = sentence_to_indices(w2i, sentence)\n",
    "    indices.append(w2i[\"<eos>\"])\n",
    "    return torch.tensor(indices, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "\n",
    "train_english = read_data(\"data/train_preprocessed.en\")\n",
    "train_french = read_data(\"data/train_preprocessed.fr\")\n",
    "\n",
    "val_english = read_data(\"data/val_preprocessed.en\")\n",
    "val_french = read_data(\"data/val_preprocessed.fr\")\n",
    "\n",
    "w2i_french = word_to_index(\"data/train_preprocessed.fr.json\")\n",
    "w2i_english = word_to_index(\"data/train_preprocessed.en.json\")\n",
    "\n",
    "i2w_french = index_to_word(w2i_french)\n",
    "i2w_english = index_to_word(w2i_english)\n",
    "\n",
    "EOS_token_en = w2i_english[\"<eos>\"]\n",
    "SOS_token_en = w2i_english[\"<sos>\"]\n",
    "\n",
    "EOS_token_fr = w2i_french[\"<eos>\"]\n",
    "SOS_token_fr = w2i_french[\"<sos>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1% (1 of 100) |                         | Elapsed Time: 0:00:00 ETA:  0:00:18"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |#######################| Elapsed Time: 0:00:08 Time: 0:00:08\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 17 is out of bounds for dimension 0 with size 17",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-06ebd49aa14d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-06ebd49aa14d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mnmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNMT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi2w_english\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2w_french\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     train_and_evaluate(w2i_english, w2i_french, train_english, \n\u001b[0;32m--> 158\u001b[0;31m                        train_french, nmt, num_epochs, learning_rate)    \n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-06ebd49aa14d>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(w2i_english, w2i_french, train_english, train_french, nmt, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m    139\u001b[0m                                    nmt, learning_rate)\n\u001b[1;32m    140\u001b[0m         predictions, val_loss = evaluate_dataset(w2i_french, i2w_english,\n\u001b[0;32m--> 141\u001b[0;31m                                                  nmt, val_french, val_english)\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-06ebd49aa14d>\u001b[0m in \u001b[0;36mevaluate_dataset\u001b[0;34m(w2i_french, i2w_english, nmt, val_french, val_english)\u001b[0m\n\u001b[1;32m     70\u001b[0m         predicted_words, attentions, loss = nmt.evaluate(w2i_french, i2w_english, \n\u001b[1;32m     71\u001b[0m                                                          \u001b[0minput_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                                                          criterion)\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mpredicted_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-6d9f1c8852cf>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, w2i_french, i2w_english, input_sentence, target_sentence, criterion)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0mdecoded_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi2w_english\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 17 is out of bounds for dimension 0 with size 17"
     ]
    }
   ],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))    \n",
    "\n",
    "\n",
    "def train(input_sentence, target_sentence, w2i_english, \n",
    "          w2i_french, nmt, nmt_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Does one iteration of training.\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = 0     \n",
    "    output_sentence = []\n",
    "    input_tensor = sentence_to_tensor(w2i_french, input_sentence)\n",
    "    target_tensor = sentence_to_tensor(w2i_english, target_sentence)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    nmt_optimizer.zero_grad()\n",
    "    loss = nmt.forward(input_tensor, target_tensor)\n",
    "    \n",
    "    nmt_optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_dataset(w2i_english, w2i_french, train_english, \n",
    "                  train_french, nmt, learning_rate):\n",
    "    \"\"\"\n",
    "    Trains the Encoder-Decoder model for the entire data set.\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    nmt_optimizer = optim.SGD(nmt.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0\n",
    "\n",
    "#     with progressbar.ProgressBar(max_value=len(train_english)) as bar:\n",
    "    with progressbar.ProgressBar(max_value=100) as bar:\n",
    "#         for iter in range(1, len(train_english) + 1):\n",
    "        for iter in range(1, 100+1):\n",
    "            input_sentence = train_french[iter-1]\n",
    "            target_sentence = train_english[iter-1]\n",
    "            loss = train(input_sentence, target_sentence, w2i_english, \n",
    "                         w2i_french, nmt, nmt_optimizer, criterion)\n",
    "            total_loss += loss\n",
    "            bar.update(iter-1)\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def evaluate_dataset(w2i_french, i2w_english, nmt, val_french, val_english):\n",
    "    \"\"\"\n",
    "    Evaluates the data set and returns the obtained predictions and loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = []\n",
    "\n",
    "    for i, input_sentence in enumerate(val_french):\n",
    "        target_sentence = val_english[i]\n",
    "        \n",
    "        predicted_words, attentions = nmt.evaluate(w2i_french, i2w_english, \n",
    "                                                   input_sentence, target_sentence)\n",
    "        predicted_sentence = ' '.join(predicted_words)\n",
    "        predictions.append(predicted_sentence)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def save_model(model, positional, epoch, lr, train_loss, val_loss):\n",
    "    \"\"\"\n",
    "    Saves the NMT model to a file.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_type = \"rnn\"\n",
    "    if positional: model_type = \"pos\"\n",
    "        \n",
    "    path = model_type + \"_epoch\" + str(epoch) + \"_lr\" + str(lr) + \"_trainloss\" + \\\n",
    "        str(train_loss) + \"_valloss \" + str(val_loss)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    \n",
    "\n",
    "def save_predictions(predictions, epoch, lr, train_loss, val_loss):\n",
    "    \"\"\"\n",
    "    Saves the encoded predicted sentences decoded to a file.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_type = \"rnn\"\n",
    "    if positional: model_type = \"pos\"\n",
    "        \n",
    "    path = \"predictions_\" + model_type + \"_epoch\" + str(epoch) + \"_lr\" + str(lr) + \\\n",
    "        \"_trainloss\" + str(train_loss) + \"_valloss \" + str(val_loss) + \".txt\"\n",
    "    \n",
    "    with open(\"temp_encoded.txt\", \"w\") as f:\n",
    "        for sentence in predictions:\n",
    "            f.write(sentence + \"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    command = \"sed -r 's/(@@ )|(@@ ?$)//g' temp_encoded.txt > \" + path\n",
    "    os.system(command)\n",
    "    os.remove(\"temp_encoded.txt\")\n",
    "    \n",
    "\n",
    "def save_losses(train_loss, val_loss):\n",
    "    \"\"\"\n",
    "    Appends the obtained losses to a file.\n",
    "    \"\"\"\n",
    "    \n",
    "    path = model_type + \"_epoch\" + str(epoch) + \"_lr\" + str(lr)\n",
    "    \n",
    "    with open(\"train_losses_\" + path, \"a\") as train_file:\n",
    "        train_file.write(str(train_loss) + \"\\n\")\n",
    "    train_file.close()\n",
    "    \n",
    "    with open(\"val_losses_\" + path, \"a\") as val_file:\n",
    "        val_file.write(str(val_loss) + \"\\n\")\n",
    "    val_file.close()\n",
    "    \n",
    "\n",
    "def train_and_evaluate(w2i_english, w2i_french, train_english, train_french,\n",
    "                       nmt, num_epochs, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Trains the Encoder-Decoder for a certain amount of epochs.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for iter in range(1, num_epochs + 1):\n",
    "        print(\"Iteration\", iter, \"of\", num_epochs)\n",
    "        train_loss = train_dataset(w2i_english, w2i_french,\n",
    "                                   train_english, train_french, \n",
    "                                   nmt, learning_rate)\n",
    "        predictions = evaluate_dataset(w2i_french, i2w_english,\n",
    "                                       nmt, val_french, val_english)\n",
    "        print(\"training loss:\", train_loss)\n",
    "        print(\"validation loss:\", val_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        save_model(nmt, positional, iter, learning_rate, train_loss, val_loss)\n",
    "        save_predictions(predictions, iter, learning_rate, train_loss, val_loss)\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    positional = True\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    nmt = NMT(i2w_english, i2w_french, 256, positional, criterion)\n",
    "    train_and_evaluate(w2i_english, w2i_french, train_english, \n",
    "                       train_french, nmt, num_epochs, learning_rate)    \n",
    "    \n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
