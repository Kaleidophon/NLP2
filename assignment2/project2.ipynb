{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import math\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "\n",
    "class EncoderRNN(nn.Module): \n",
    "    def __init__(self, input_size, hidden_size, positional, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.positional = positional\n",
    "        self.word_embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.pos_embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        word_embedded = self.word_embedding(input).view(1, 1, -1)\n",
    "        \n",
    "        if self.positional:\n",
    "            pos_embedded = self.pos_embedding(input).view(1, 1, -1)\n",
    "            output = self.dropout(word_embedded + pos_embedded)\n",
    "            output = self.linear(output)\n",
    "            hidden = self.linear(hidden)\n",
    "        else:\n",
    "            output, hidden = self.gru(word_embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "    \n",
    "    \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    \"\"\"\n",
    "    Reads the data and returns it in a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    f = open(file_name, \"r\")\n",
    "    return [line.strip().split() for line in f.readlines()]\n",
    "\n",
    "\n",
    "def word_to_index(file_name):\n",
    "    \"\"\"\n",
    "    Obtains the vocabulary of a file and returns it \n",
    "    in a dictionary to be able to use w2i.\n",
    "    \"\"\"\n",
    "    \n",
    "    file = open(file_name) \n",
    "    w2i = json.load(file)\n",
    "    w2i[\"sos\"] = len(w2i)\n",
    "    return w2i\n",
    "\n",
    "\n",
    "def index_to_word(dictionary):\n",
    "    \"\"\"\n",
    "    Reverses the dictionary such that i2w can be used.\n",
    "    \"\"\"\n",
    "    \n",
    "    reversed_dict = {}\n",
    "    \n",
    "    for word, index in dictionary.items():\n",
    "        reversed_dict[index] = word\n",
    "    reversed_dict[index + 1] = \"sos\" \n",
    "    return reversed_dict\n",
    "\n",
    "\n",
    "def sentence_to_indices(w2i, sentence):\n",
    "    \"\"\"\n",
    "    Returns the indices of the words in a sentence in a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [w2i[word] for word in sentence]\n",
    "\n",
    "\n",
    "def sentence_to_tensor(w2i, sentence):\n",
    "    \"\"\"\n",
    "    Returns the tensor of a sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    indices = sentence_to_indices(w2i, sentence)\n",
    "    indices.append(EOS_token)\n",
    "    return torch.tensor(indices, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "train_english = read_data(\"data/train_preprocessed.en\")\n",
    "train_french = read_data(\"data/train_preprocessed.fr\")\n",
    "\n",
    "w2i_french = word_to_index(\"data/train_preprocessed.fr.json\")\n",
    "w2i_english = word_to_index(\"data/train_preprocessed.en.json\")\n",
    "\n",
    "i2w_french = index_to_word(w2i_french)\n",
    "i2w_english = index_to_word(w2i_english)\n",
    "\n",
    "EOS_token = w2i_english[\"eos\"]\n",
    "SOS_token = w2i_english[\"sos\"]\n",
    "teacher_forcing_ratio = 0.5\n",
    "positional = True\n",
    "encoder = EncoderRNN(len(i2w_english), 256, positional)\n",
    "decoder = AttnDecoderRNN(256, len(i2w_french))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 9s (- 47m 53s) (100 0%) nan\n"
     ]
    }
   ],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))    \n",
    "    \n",
    "# def train(w2i_english, w2i_french, dataloader_english, dataloader_french, \n",
    "#           encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "#           criterion, minibatch_size, max_length=MAX_LENGTH):\n",
    "#     \"\"\"\n",
    "#     Does one iteration of training.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     loss = 0\n",
    "    \n",
    "#     for i in range(0, len(train_english), minibatch_size):\n",
    "        \n",
    "#         for j in range(minibatch_size):           \n",
    "#             print(\"Sentence\",  str(i+j), \"out of\", str(len(train_english)))\n",
    "#             input_tensor = sentence_to_tensor(w2i_english, train_english[i+j])\n",
    "#             target_tensor = sentence_to_tensor(w2i_french, train_french[i+j])\n",
    "\n",
    "#             encoder_hidden = encoder.initHidden()\n",
    "\n",
    "#             encoder_optimizer.zero_grad()\n",
    "#             decoder_optimizer.zero_grad()\n",
    "\n",
    "#             input_length = input_tensor.size(0)\n",
    "#             target_length = target_tensor.size(0)\n",
    "\n",
    "#             encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "\n",
    "#             for ei in range(input_length):\n",
    "#                 encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "#                 encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "#             decoder_input = torch.tensor([[SOS_token]])\n",
    "#             decoder_hidden = encoder_hidden\n",
    "#             use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "#             if use_teacher_forcing:\n",
    "#                 # Teacher forcing: Feed the target as the next input\n",
    "#                 for di in range(target_length):\n",
    "#                     decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                         decoder_input, decoder_hidden, encoder_outputs)\n",
    "#                     loss += criterion(decoder_output, target_tensor[di])\n",
    "#                     decoder_input = target_tensor[di]  # Teacher forcing\n",
    "#             else:\n",
    "#                 # Without teacher forcing: use its own predictions as the next input\n",
    "#                 for di in range(target_length):\n",
    "#                     decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                         decoder_input, decoder_hidden, encoder_outputs)\n",
    "#                     topv, topi = decoder_output.topk(1)\n",
    "#                     decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "#                     loss += criterion(decoder_output, target_tensor[di])\n",
    "#                     if decoder_input.item() == EOS_token:\n",
    "#                         break\n",
    "\n",
    "# #             loss.backward(retain_graph=True)            \n",
    "#             loss.backward()            \n",
    "\n",
    "#             encoder_optimizer.step()\n",
    "#             decoder_optimizer.step()\n",
    "#         encoder_hidden = encoder_hidden.detach()\n",
    "#         decoder_hidden = decoder_hidden.detach()\n",
    "#     return loss.item()/target_length\n",
    "\n",
    "\n",
    "def train(input_sentence, target_sentence, w2i_english, \n",
    "          w2i_french, encoder, decoder, encoder_optimizer, \n",
    "          decoder_optimizer, criterion, minibatch_size, \n",
    "          max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Does one iteration of training.\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = 0     \n",
    "    input_tensor = sentence_to_tensor(w2i_english, input_sentence)\n",
    "    target_tensor = sentence_to_tensor(w2i_french, target_sentence)\n",
    "\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "\n",
    "    if input_length > MAX_LENGTH: input_length = MAX_LENGTH\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]])\n",
    "    decoder_hidden = encoder_hidden\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break         \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.item()/target_length\n",
    "\n",
    "\n",
    "def train_iterations(w2i_english, w2i_french, train_english, train_french,\n",
    "                     encoder, decoder, minibatch_size, n_iters, print_every=100, \n",
    "                     plot_every=100, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Trains the Encoder-Decoder model for a certain amount of iterations.\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        input_sentence = train_english[iter-1]\n",
    "        target_sentence = train_french[iter-1]\n",
    "        loss = train(input_sentence, target_sentence, w2i_english, w2i_french,\n",
    "                     encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "                     criterion, minibatch_size)        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter/n_iters),\n",
    "                                         iter, float(iter)/n_iters*100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    showPlot(plot_losses)\n",
    "    \n",
    "    \n",
    "# trainloader_english = torch.utils.data.DataLoader(train_english, \n",
    "#                                                   batch_size=32, shuffle=False, \n",
    "#                                                   num_workers=8)\n",
    "# trainloader_french = torch.utils.data.DataLoader(train_french,\n",
    "#                                                 batch_size=32, shuffle=False,\n",
    "#                                                 num_workers=8)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=8)\n",
    "    \n",
    "train_iterations(w2i_english, w2i_french, train_english, train_french,\n",
    "                 encoder, decoder, 10, 29000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
