{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import os\n",
    "import progressbar\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "\n",
    "class EncoderRNN(nn.Module): \n",
    "    def __init__(self, input_size, hidden_size, positional, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.positional = positional\n",
    "        self.word_embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.pos_embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        word_embedded = self.word_embedding(input).view(1, 1, -1)\n",
    "        \n",
    "        if self.positional:\n",
    "            pos_embedded = self.pos_embedding(input).view(1, 1, -1)\n",
    "            output = self.dropout(word_embedded + pos_embedded)\n",
    "            output = self.linear(output)\n",
    "            hidden = self.linear(hidden)\n",
    "        else:\n",
    "            output, hidden = self.gru(word_embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "    \n",
    "    \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    \"\"\"\n",
    "    Reads the data and returns it in a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    f = open(file_name, \"r\")\n",
    "    return [line.strip().split() for line in f.readlines()]\n",
    "\n",
    "\n",
    "def word_to_index(file_name):\n",
    "    \"\"\"\n",
    "    Obtains the vocabulary of a file and returns it \n",
    "    in a dictionary to be able to use w2i.\n",
    "    \"\"\"\n",
    "    \n",
    "    file = open(file_name) \n",
    "    w2i = json.load(file)\n",
    "    w2i[\"<sos>\"] = len(w2i)\n",
    "    return w2i\n",
    "\n",
    "\n",
    "def index_to_word(dictionary):\n",
    "    \"\"\"\n",
    "    Reverses the dictionary such that i2w can be used.\n",
    "    \"\"\"\n",
    "    \n",
    "    reversed_dict = {}\n",
    "    \n",
    "    for word, index in dictionary.items():\n",
    "        reversed_dict[index] = word\n",
    "    reversed_dict[index + 1] = \"<sos>\" \n",
    "    return reversed_dict\n",
    "\n",
    "\n",
    "def sentence_to_indices(w2i, sentence):\n",
    "    \"\"\"\n",
    "    Returns the indices of the words in a sentence in a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [w2i[word] for word in sentence]\n",
    "\n",
    "\n",
    "def sentence_to_tensor(w2i, sentence):\n",
    "    \"\"\"\n",
    "    Returns the tensor of a sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    indices = sentence_to_indices(w2i, sentence)\n",
    "    indices.append(EOS_token)\n",
    "    return torch.tensor(indices, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "train_english = read_data(\"data/train_preprocessed.en\")\n",
    "train_french = read_data(\"data/train_preprocessed.fr\")\n",
    "\n",
    "val_english = read_data(\"data/val_preprocessed.en\")\n",
    "val_french = read_data(\"data/val_preprocessed.fr\")\n",
    "\n",
    "w2i_french = word_to_index(\"data/train_preprocessed.fr.json\")\n",
    "w2i_english = word_to_index(\"data/train_preprocessed.en.json\")\n",
    "\n",
    "i2w_french = index_to_word(w2i_french)\n",
    "i2w_english = index_to_word(w2i_english)\n",
    "\n",
    "EOS_token = w2i_english[\"<eos>\"]\n",
    "SOS_token = w2i_english[\"<sos>\"]\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "positional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      "N/A% (0 of 1000) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |#####################| Elapsed Time: 0:01:20 Time: 0:01:20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4064.98785898142\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIhKJQAiIiqVAIiIqFQCICKiUv8PZGDRL/nhiAUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f12fe444ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "#     fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "#     loc = ticker.MultipleLocator(base=0.2)\n",
    "#     ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))    \n",
    "\n",
    "\n",
    "def train(input_sentence, target_sentence, w2i_english, \n",
    "          w2i_french, encoder, decoder, encoder_optimizer, \n",
    "          decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Does one iteration of training.\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = 0     \n",
    "    output_sentence = []\n",
    "    input_tensor = sentence_to_tensor(w2i_english, input_sentence)\n",
    "    target_tensor = sentence_to_tensor(w2i_french, target_sentence)\n",
    "\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "\n",
    "    if input_length > MAX_LENGTH: input_length = MAX_LENGTH\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]])\n",
    "    decoder_hidden = encoder_hidden\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break         \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.item()/target_length\n",
    "\n",
    "\n",
    "def train_dataset(w2i_english, w2i_french, train_english, \n",
    "                  train_french, encoder, decoder, learning_rate):\n",
    "    \"\"\"\n",
    "    Trains the Encoder-Decoder model for the entire data set.\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0\n",
    "\n",
    "#     with progressbar.ProgressBar(max_value=len(train_english)) as bar:\n",
    "    with progressbar.ProgressBar(max_value=1000) as bar:\n",
    "#         for iter in range(1, len(train_english) + 1):\n",
    "        for iter in range(1, 1000+1):\n",
    "            input_sentence = train_english[iter-1]\n",
    "            target_sentence = train_french[iter-1]\n",
    "            loss = train(input_sentence, target_sentence, w2i_english, \n",
    "                         w2i_french, encoder, decoder, encoder_optimizer, \n",
    "                         decoder_optimizer, criterion)\n",
    "            total_loss += loss\n",
    "            bar.update(iter-1)\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def train_epochs(w2i_english, w2i_french, train_english, train_french,\n",
    "                encoder, decoder, num_epochs, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Trains the Encoder-Decoder for a certain amount of epochs.\n",
    "    \"\"\"\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for iter in range(1, num_epochs + 1):\n",
    "        print(\"Iteration\", iter, \"of\", num_epochs)\n",
    "        loss = train_dataset(w2i_english, w2i_french,\n",
    "                             train_english, train_french, \n",
    "                             encoder, decoder, learning_rate)\n",
    "        print(\"loss:\", loss)\n",
    "        losses.append(loss)\n",
    "    showPlot(losses)\n",
    "    \n",
    "encoder = EncoderRNN(len(i2w_english), 256, positional)\n",
    "decoder = AttnDecoderRNN(256, len(i2w_french))\n",
    "train_epochs(w2i_english, w2i_french, train_english, train_french,\n",
    "             encoder, decoder, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, w2i_english, i2w_french,\n",
    "             sentence, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Evaluates a sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_tensor = sentence_to_tensor(w2i_english, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "\n",
    "        if input_length > MAX_LENGTH: input_length = MAX_LENGTH\n",
    "        \n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]])  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append(\"<eos>\")\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(i2w_french[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "    \n",
    "    \n",
    "def evaluate_dataset(encoder, decoder, val_english):\n",
    "    \"\"\"\n",
    "    Evaluates the entire validation set.\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for sentence in val_english:\n",
    "        predicted_words, attentions = evaluate(encoder, decoder, w2i_english, i2w_french, sentence)\n",
    "        predicted_sentence = ' '.join(predicted_words)\n",
    "        predictions.append(predicted_sentence)\n",
    "    return predictions\n",
    "        \n",
    "\n",
    "def save_predictions(predictions, file_name):\n",
    "    \"\"\"\n",
    "    Saves the encoded predicted sentences decoded to a file.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(\"temp_encoded.txt\", \"w\") as f:\n",
    "        for sentence in predictions:\n",
    "            f.write(sentence + \"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    os.system(f\"sed -r 's/(@@ )|(@@ ?$)//g' temp_encoded.txt > {file_name}\")\n",
    "    os.remove(\"temp_encoded.txt\")\n",
    "    \n",
    "\n",
    "predictions = evaluate_dataset(encoder, decoder, val_english)\n",
    "save_predictions(predictions, \"test_val.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
